{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb608484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Try to import XGBoost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "    print('XGBoost is available')\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print('XGBoost not installed. Will skip XGBoost model.')\n",
    "    print('To install: pip install xgboost')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print('Libraries imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61181636",
   "metadata": {},
   "source": [
    "## 1. Load Data and Prepare Train/Test Split\n",
    "\n",
    "Load features and create the same train/test split as previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcbad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features\n",
    "PROCESSED = os.path.abspath(os.path.join('..', 'data', 'processed'))\n",
    "features_path = os.path.join(PROCESSED, 'features.csv')\n",
    "\n",
    "if not os.path.exists(features_path):\n",
    "    print(f'ERROR: features.csv not found at {features_path}')\n",
    "else:\n",
    "    df = pd.read_csv(features_path)\n",
    "    print(f'Features loaded successfully')\n",
    "    print(f'Dataset shape: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc51d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify target column\n",
    "possible_target_names = ['is_laundering', 'is_fraud', 'label', 'target', 'fraud', 'laundering']\n",
    "target_col = None\n",
    "\n",
    "for col in possible_target_names:\n",
    "    if col in df.columns:\n",
    "        target_col = col\n",
    "        break\n",
    "\n",
    "print(f'Target column: {target_col}')\n",
    "print(f'\\nClass distribution:')\n",
    "print(df[target_col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd80c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "exclude_cols = [target_col, 'id', 'transaction_id', 'account_id', 'customer_id']\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[target_col]\n",
    "\n",
    "print(f'Features shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eca967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f'Training set: {X_train.shape[0]} samples')\n",
    "print(f'Test set: {X_test.shape[0]} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b275d8d2",
   "metadata": {},
   "source": [
    "## 2. Random Forest Model\n",
    "\n",
    "Random Forest is an ensemble of decision trees that reduces overfitting and improves accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4b4d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest with class balancing\n",
    "print('Training Random Forest...')\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "print('Training complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f24de51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "rf_precision = precision_score(y_test, y_pred_rf)\n",
    "rf_recall = recall_score(y_test, y_pred_rf)\n",
    "rf_f1 = f1_score(y_test, y_pred_rf)\n",
    "rf_roc_auc = roc_auc_score(y_test, y_pred_proba_rf)\n",
    "\n",
    "print('=' * 60)\n",
    "print('RANDOM FOREST - EVALUATION METRICS')\n",
    "print('=' * 60)\n",
    "print(f'Accuracy:  {rf_accuracy:.4f}')\n",
    "print(f'Precision: {rf_precision:.4f}')\n",
    "print(f'Recall:    {rf_recall:.4f}')\n",
    "print(f'F1 Score:  {rf_f1:.4f}')\n",
    "print(f'ROC-AUC:   {rf_roc_auc:.4f}')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6850842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print('\\nDetailed Classification Report:')\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5999e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "rf_cm = confusion_matrix(y_test, y_pred_rf)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix - Random Forest')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nConfusion Matrix:')\n",
    "print(f'True Negatives:  {rf_cm[0, 0]}')\n",
    "print(f'False Positives: {rf_cm[0, 1]}')\n",
    "print(f'False Negatives: {rf_cm[1, 0]}')\n",
    "print(f'True Positives:  {rf_cm[1, 1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f79831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print('\\nTop 10 Most Important Features:')\n",
    "print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Plot top features\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features = feature_importance.head(10)\n",
    "plt.barh(top_features['Feature'], top_features['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 10 Feature Importances - Random Forest')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54db79bf",
   "metadata": {},
   "source": [
    "## 3. XGBoost Model\n",
    "\n",
    "XGBoost is a powerful gradient boosting algorithm. We'll train it if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5eae5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if XGBOOST_AVAILABLE:\n",
    "    # Calculate scale_pos_weight for class imbalance\n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    print(f'Scale pos weight: {scale_pos_weight:.2f}')\n",
    "    \n",
    "    # Train XGBoost\n",
    "    print('\\nTraining XGBoost...')\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    print('Training complete.')\n",
    "else:\n",
    "    print('XGBoost is not available. Skipping this model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17725007",
   "metadata": {},
   "outputs": [],
   "source": [
    "if XGBOOST_AVAILABLE:\n",
    "    # Make predictions\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "    y_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "    xgb_precision = precision_score(y_test, y_pred_xgb)\n",
    "    xgb_recall = recall_score(y_test, y_pred_xgb)\n",
    "    xgb_f1 = f1_score(y_test, y_pred_xgb)\n",
    "    xgb_roc_auc = roc_auc_score(y_test, y_pred_proba_xgb)\n",
    "    \n",
    "    print('=' * 60)\n",
    "    print('XGBOOST - EVALUATION METRICS')\n",
    "    print('=' * 60)\n",
    "    print(f'Accuracy:  {xgb_accuracy:.4f}')\n",
    "    print(f'Precision: {xgb_precision:.4f}')\n",
    "    print(f'Recall:    {xgb_recall:.4f}')\n",
    "    print(f'F1 Score:  {xgb_f1:.4f}')\n",
    "    print(f'ROC-AUC:   {xgb_roc_auc:.4f}')\n",
    "    print('=' * 60)\n",
    "else:\n",
    "    print('XGBoost is not available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91919be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if XGBOOST_AVAILABLE:\n",
    "    # Classification report\n",
    "    print('\\nDetailed Classification Report:')\n",
    "    print(classification_report(y_test, y_pred_xgb))\n",
    "else:\n",
    "    print('XGBoost is not available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9df10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if XGBOOST_AVAILABLE:\n",
    "    # Confusion Matrix\n",
    "    xgb_cm = confusion_matrix(y_test, y_pred_xgb)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(xgb_cm, annot=True, fmt='d', cmap='Greens', cbar=False)\n",
    "    plt.title('Confusion Matrix - XGBoost')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'\\nConfusion Matrix:')\n",
    "    print(f'True Negatives:  {xgb_cm[0, 0]}')\n",
    "    print(f'False Positives: {xgb_cm[0, 1]}')\n",
    "    print(f'False Negatives: {xgb_cm[1, 0]}')\n",
    "    print(f'True Positives:  {xgb_cm[1, 1]}')\n",
    "else:\n",
    "    print('XGBoost is not available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3056f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "if XGBOOST_AVAILABLE:\n",
    "    # Feature importance for XGBoost\n",
    "    xgb_feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Importance': xgb_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print('\\nTop 10 Most Important Features (XGBoost):')\n",
    "    print(xgb_feature_importance.head(10).to_string(index=False))\n",
    "    \n",
    "    # Plot top features\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_features_xgb = xgb_feature_importance.head(10)\n",
    "    plt.barh(top_features_xgb['Feature'], top_features_xgb['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Top 10 Feature Importances - XGBoost')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('XGBoost is not available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cd6bea",
   "metadata": {},
   "source": [
    "## 4. ROC Curves\n",
    "\n",
    "Plot ROC curves to visualize the trade-off between true positive rate and false positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363fb20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curves\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {rf_roc_auc:.4f})', linewidth=2)\n",
    "\n",
    "if XGBOOST_AVAILABLE:\n",
    "    fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_proba_xgb)\n",
    "    plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC = {xgb_roc_auc:.4f})', linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - Advanced Models')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56630208",
   "metadata": {},
   "source": [
    "## 5. Compare Advanced Models\n",
    "\n",
    "Side-by-side comparison of Random Forest and XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b617c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "if XGBOOST_AVAILABLE:\n",
    "    comparison = pd.DataFrame({\n",
    "        'Model': ['Random Forest', 'XGBoost'],\n",
    "        'Accuracy': [rf_accuracy, xgb_accuracy],\n",
    "        'Precision': [rf_precision, xgb_precision],\n",
    "        'Recall': [rf_recall, xgb_recall],\n",
    "        'F1 Score': [rf_f1, xgb_f1],\n",
    "        'ROC-AUC': [rf_roc_auc, xgb_roc_auc]\n",
    "    })\n",
    "else:\n",
    "    comparison = pd.DataFrame({\n",
    "        'Model': ['Random Forest'],\n",
    "        'Accuracy': [rf_accuracy],\n",
    "        'Precision': [rf_precision],\n",
    "        'Recall': [rf_recall],\n",
    "        'F1 Score': [rf_f1],\n",
    "        'ROC-AUC': [rf_roc_auc]\n",
    "    })\n",
    "\n",
    "print('\\n' + '=' * 85)\n",
    "print('ADVANCED MODELS COMPARISON')\n",
    "print('=' * 85)\n",
    "print(comparison.to_string(index=False))\n",
    "print('=' * 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721762bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "if XGBOOST_AVAILABLE:\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
    "    rf_scores = [rf_accuracy, rf_precision, rf_recall, rf_f1, rf_roc_auc]\n",
    "    xgb_scores = [xgb_accuracy, xgb_precision, xgb_recall, xgb_f1, xgb_roc_auc]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(x - width/2, rf_scores, width, label='Random Forest', alpha=0.8)\n",
    "    ax.bar(x + width/2, xgb_scores, width, label='XGBoost', alpha=0.8)\n",
    "    \n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Advanced Models Performance Comparison')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics)\n",
    "    ax.legend()\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Only Random Forest is available for comparison.')\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
    "    rf_scores = [rf_accuracy, rf_precision, rf_recall, rf_f1, rf_roc_auc]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.bar(metrics, rf_scores, alpha=0.8, color='steelblue')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Random Forest Performance')\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ac5bb5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Advanced Models Performance:\n",
    "\n",
    "**Random Forest:**\n",
    "- Ensemble of decision trees that reduces overfitting\n",
    "- Uses class weighting to handle imbalance\n",
    "- Provides feature importance rankings\n",
    "- Generally more robust than single decision trees\n",
    "\n",
    "**XGBoost** (if available):\n",
    "- Gradient boosting algorithm - builds trees sequentially\n",
    "- Often achieves state-of-the-art performance\n",
    "- Uses `scale_pos_weight` to handle class imbalance\n",
    "- More computationally intensive but often more accurate\n",
    "\n",
    "### Key Observations:\n",
    "\n",
    "1. **ROC-AUC** provides a threshold-independent measure of model performance\n",
    "2. **Feature importance** helps understand which features drive predictions\n",
    "3. Both models typically outperform simple baselines (Logistic Regression, Decision Tree)\n",
    "4. For AML detection, focus on **recall** - catching fraud cases is critical\n",
    "\n",
    "### Next Steps:\n",
    "- Optimize decision thresholds to improve recall\n",
    "- Compare all models (baseline + advanced) in one place\n",
    "- Select the best model for final deployment"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
