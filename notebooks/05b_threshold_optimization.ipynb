{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82857f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    precision_recall_curve, roc_curve, roc_auc_score\n",
    ")\n",
    "\n",
    "# Try to import XGBoost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print('XGBoost not available. Will use Random Forest only.')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print('Libraries imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630155b4",
   "metadata": {},
   "source": [
    "## 1. Load Data and Train Model\n",
    "\n",
    "Load features and train a model (we'll use Random Forest or XGBoost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5987fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features\n",
    "PROCESSED = os.path.abspath(os.path.join('..', 'data', 'processed'))\n",
    "features_path = os.path.join(PROCESSED, 'features.csv')\n",
    "\n",
    "if not os.path.exists(features_path):\n",
    "    print(f'ERROR: features.csv not found at {features_path}')\n",
    "else:\n",
    "    df = pd.read_csv(features_path)\n",
    "    print(f'Features loaded successfully')\n",
    "    print(f'Dataset shape: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144ab663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify target column\n",
    "possible_target_names = ['is_laundering', 'is_fraud', 'label', 'target', 'fraud', 'laundering']\n",
    "target_col = None\n",
    "\n",
    "for col in possible_target_names:\n",
    "    if col in df.columns:\n",
    "        target_col = col\n",
    "        break\n",
    "\n",
    "print(f'Target column: {target_col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26ed7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "exclude_cols = [target_col, 'id', 'transaction_id', 'account_id', 'customer_id']\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[target_col]\n",
    "\n",
    "print(f'Features shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97792969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f'Training set: {X_train.shape[0]} samples')\n",
    "print(f'Test set: {X_test.shape[0]} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb13fb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model (prefer XGBoost if available, otherwise Random Forest)\n",
    "if XGBOOST_AVAILABLE:\n",
    "    print('Training XGBoost model...')\n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    model_name = 'XGBoost'\n",
    "else:\n",
    "    print('Training Random Forest model...')\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=15,\n",
    "        random_state=42,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model_name = 'Random Forest'\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(f'{model_name} training complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7898c22f",
   "metadata": {},
   "source": [
    "## 2. Get Prediction Probabilities\n",
    "\n",
    "Get probability scores instead of hard predictions to enable threshold tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d61c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction probabilities for positive class (fraud/laundering)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Default predictions using 0.5 threshold\n",
    "y_pred_default = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "print(f'Probability scores (first 10): {y_pred_proba[:10]}')\n",
    "print(f'\\nProbability statistics:')\n",
    "print(f'Min:  {y_pred_proba.min():.4f}')\n",
    "print(f'Max:  {y_pred_proba.max():.4f}')\n",
    "print(f'Mean: {y_pred_proba.mean():.4f}')\n",
    "print(f'Median: {np.median(y_pred_proba):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230901ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of prediction probabilities\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(y_pred_proba[y_test == 0], bins=50, alpha=0.5, label='Non-Fraud', color='blue')\n",
    "plt.hist(y_pred_proba[y_test == 1], bins=50, alpha=0.5, label='Fraud', color='red')\n",
    "plt.axvline(x=0.5, color='black', linestyle='--', label='Default Threshold (0.5)')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Predicted Probabilities')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3601a9",
   "metadata": {},
   "source": [
    "## 3. Baseline Performance (Default Threshold = 0.5)\n",
    "\n",
    "First, evaluate performance using the default 0.5 threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4279d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics with default threshold\n",
    "default_accuracy = accuracy_score(y_test, y_pred_default)\n",
    "default_precision = precision_score(y_test, y_pred_default)\n",
    "default_recall = recall_score(y_test, y_pred_default)\n",
    "default_f1 = f1_score(y_test, y_pred_default)\n",
    "\n",
    "print('=' * 60)\n",
    "print(f'{model_name} - DEFAULT THRESHOLD (0.5)')\n",
    "print('=' * 60)\n",
    "print(f'Accuracy:  {default_accuracy:.4f}')\n",
    "print(f'Precision: {default_precision:.4f}')\n",
    "print(f'Recall:    {default_recall:.4f}')\n",
    "print(f'F1 Score:  {default_f1:.4f}')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cf3162",
   "metadata": {},
   "source": [
    "## 4. Precision-Recall Curve\n",
    "\n",
    "Plot the trade-off between precision and recall at different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9222cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision-recall curve\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision, linewidth=2, label='Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Number of thresholds evaluated: {len(pr_thresholds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a170b46",
   "metadata": {},
   "source": [
    "## 5. ROC Curve\n",
    "\n",
    "Plot the Receiver Operating Characteristic curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e803be43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.title('ROC Curve')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'ROC-AUC Score: {roc_auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbc04e4",
   "metadata": {},
   "source": [
    "## 6. Find Optimal Threshold Based on F1 Score\n",
    "\n",
    "Test multiple thresholds and find the one that maximizes F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b50c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test thresholds from 0.1 to 0.9\n",
    "thresholds_to_test = np.arange(0.1, 0.91, 0.05)\n",
    "results = []\n",
    "\n",
    "for threshold in thresholds_to_test:\n",
    "    y_pred_thresh = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred_thresh)\n",
    "    prec = precision_score(y_test, y_pred_thresh, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred_thresh)\n",
    "    f1 = f1_score(y_test, y_pred_thresh, zero_division=0)\n",
    "    \n",
    "    results.append({\n",
    "        'Threshold': threshold,\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1 Score': f1\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print('\\nMetrics at Different Thresholds:')\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f4fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best threshold based on F1 score\n",
    "best_f1_idx = results_df['F1 Score'].idxmax()\n",
    "best_threshold = results_df.loc[best_f1_idx, 'Threshold']\n",
    "best_f1 = results_df.loc[best_f1_idx, 'F1 Score']\n",
    "\n",
    "print(f'\\n' + '=' * 60)\n",
    "print(f'BEST THRESHOLD FOR F1 SCORE: {best_threshold:.2f}')\n",
    "print('=' * 60)\n",
    "print(f'Best F1 Score: {best_f1:.4f}')\n",
    "print('\\nMetrics at best threshold:')\n",
    "print(results_df.loc[best_f1_idx].to_string())\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeda0633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics across thresholds\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(results_df['Threshold'], results_df['Precision'], marker='o', label='Precision')\n",
    "plt.plot(results_df['Threshold'], results_df['Recall'], marker='s', label='Recall')\n",
    "plt.plot(results_df['Threshold'], results_df['F1 Score'], marker='^', label='F1 Score', linewidth=2)\n",
    "plt.axvline(x=best_threshold, color='red', linestyle='--', label=f'Best Threshold ({best_threshold:.2f})')\n",
    "plt.axvline(x=0.5, color='gray', linestyle=':', label='Default Threshold (0.5)')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Metrics vs Threshold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0276f88e",
   "metadata": {},
   "source": [
    "## 7. Alternative: Find Threshold for Maximum Recall\n",
    "\n",
    "For AML detection, we might prioritize recall over F1. Find threshold that maximizes recall while maintaining reasonable precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abf0a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find threshold that gives recall >= 0.9 with highest F1\n",
    "high_recall_df = results_df[results_df['Recall'] >= 0.85]\n",
    "\n",
    "if len(high_recall_df) > 0:\n",
    "    best_recall_idx = high_recall_df['F1 Score'].idxmax()\n",
    "    recall_threshold = high_recall_df.loc[best_recall_idx, 'Threshold']\n",
    "    \n",
    "    print('=' * 60)\n",
    "    print(f'THRESHOLD FOR HIGH RECALL (≥85%): {recall_threshold:.2f}')\n",
    "    print('=' * 60)\n",
    "    print(high_recall_df.loc[best_recall_idx].to_string())\n",
    "    print('=' * 60)\n",
    "else:\n",
    "    # If no threshold gives recall >= 0.85, find one with highest recall\n",
    "    best_recall_idx = results_df['Recall'].idxmax()\n",
    "    recall_threshold = results_df.loc[best_recall_idx, 'Threshold']\n",
    "    \n",
    "    print('=' * 60)\n",
    "    print(f'THRESHOLD FOR MAXIMUM RECALL: {recall_threshold:.2f}')\n",
    "    print('=' * 60)\n",
    "    print(results_df.loc[best_recall_idx].to_string())\n",
    "    print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df9cd22",
   "metadata": {},
   "source": [
    "## 8. Confusion Matrix with Optimized Threshold\n",
    "\n",
    "Compare confusion matrices using default threshold (0.5) vs optimized threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7bf991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions with optimized threshold\n",
    "y_pred_optimized = (y_pred_proba >= best_threshold).astype(int)\n",
    "\n",
    "# Confusion matrices\n",
    "cm_default = confusion_matrix(y_test, y_pred_default)\n",
    "cm_optimized = confusion_matrix(y_test, y_pred_optimized)\n",
    "\n",
    "# Plot side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Default threshold\n",
    "sns.heatmap(cm_default, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[0])\n",
    "axes[0].set_title(f'Confusion Matrix - Default Threshold (0.5)')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "\n",
    "# Optimized threshold\n",
    "sns.heatmap(cm_optimized, annot=True, fmt='d', cmap='Greens', cbar=False, ax=axes[1])\n",
    "axes[1].set_title(f'Confusion Matrix - Optimized Threshold ({best_threshold:.2f})')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6894aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed comparison\n",
    "print('\\n' + '=' * 80)\n",
    "print('CONFUSION MATRIX COMPARISON')\n",
    "print('=' * 80)\n",
    "print(f'\\nDefault Threshold (0.5):')\n",
    "print(f'  True Negatives:  {cm_default[0, 0]:,}')\n",
    "print(f'  False Positives: {cm_default[0, 1]:,}')\n",
    "print(f'  False Negatives: {cm_default[1, 0]:,}')\n",
    "print(f'  True Positives:  {cm_default[1, 1]:,}')\n",
    "\n",
    "print(f'\\nOptimized Threshold ({best_threshold:.2f}):')\n",
    "print(f'  True Negatives:  {cm_optimized[0, 0]:,}')\n",
    "print(f'  False Positives: {cm_optimized[0, 1]:,}')\n",
    "print(f'  False Negatives: {cm_optimized[1, 0]:,}')\n",
    "print(f'  True Positives:  {cm_optimized[1, 1]:,}')\n",
    "\n",
    "print(f'\\nChanges:')\n",
    "print(f'  False Negatives: {cm_default[1, 0]:,} → {cm_optimized[1, 0]:,} (Change: {cm_optimized[1, 0] - cm_default[1, 0]:+,})')\n",
    "print(f'  True Positives:  {cm_default[1, 1]:,} → {cm_optimized[1, 1]:,} (Change: {cm_optimized[1, 1] - cm_default[1, 1]:+,})')\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b22a489",
   "metadata": {},
   "source": [
    "## 9. Final Performance with Optimized Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09b0ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics with optimized threshold\n",
    "opt_accuracy = accuracy_score(y_test, y_pred_optimized)\n",
    "opt_precision = precision_score(y_test, y_pred_optimized)\n",
    "opt_recall = recall_score(y_test, y_pred_optimized)\n",
    "opt_f1 = f1_score(y_test, y_pred_optimized)\n",
    "\n",
    "# Create comparison\n",
    "comparison = pd.DataFrame({\n",
    "    'Threshold': ['Default (0.5)', f'Optimized ({best_threshold:.2f})'],\n",
    "    'Accuracy': [default_accuracy, opt_accuracy],\n",
    "    'Precision': [default_precision, opt_precision],\n",
    "    'Recall': [default_recall, opt_recall],\n",
    "    'F1 Score': [default_f1, opt_f1]\n",
    "})\n",
    "\n",
    "print('\\n' + '=' * 90)\n",
    "print('PERFORMANCE COMPARISON: DEFAULT vs OPTIMIZED THRESHOLD')\n",
    "print('=' * 90)\n",
    "print(comparison.to_string(index=False))\n",
    "print('=' * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b8495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize improvement\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "default_scores = [default_accuracy, default_precision, default_recall, default_f1]\n",
    "optimized_scores = [opt_accuracy, opt_precision, opt_recall, opt_f1]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(x - width/2, default_scores, width, label='Default (0.5)', alpha=0.8)\n",
    "ax.bar(x + width/2, optimized_scores, width, label=f'Optimized ({best_threshold:.2f})', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Performance: Default vs Optimized Threshold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302a5d74",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "**Default Threshold (0.5):**\n",
    "- Standard approach used by most classifiers\n",
    "- May not be optimal for imbalanced problems\n",
    "- Balances precision and recall equally\n",
    "\n",
    "**Threshold Optimization:**\n",
    "- By adjusting the decision threshold, we can control the precision-recall trade-off\n",
    "- Lower threshold → Higher recall (catch more fraud) but lower precision (more false alarms)\n",
    "- Higher threshold → Higher precision (fewer false alarms) but lower recall (miss more fraud)\n",
    "\n",
    "**For AML Detection:**\n",
    "- **Recall is critical** - missing fraud cases is expensive\n",
    "- A lower threshold is often preferred to maximize fraud detection\n",
    "- False positives (flagging legitimate transactions) can be reviewed manually\n",
    "\n",
    "**Best Threshold Selection:**\n",
    "- **F1-optimal threshold**: Balances precision and recall\n",
    "- **Recall-focused threshold**: Prioritizes catching fraud cases\n",
    "- Business requirements should guide the final choice\n",
    "\n",
    "### Recommendation:\n",
    "\n",
    "Use the optimized threshold identified in this notebook for the final model. The threshold can be adjusted based on:\n",
    "- Cost of false negatives (missed fraud)\n",
    "- Cost of false positives (manual review burden)\n",
    "- Available resources for investigation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
