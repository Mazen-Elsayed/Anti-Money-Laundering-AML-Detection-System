{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ee9bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print('Libraries imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555c7416",
   "metadata": {},
   "source": [
    "## 1. Load Features Dataset\n",
    "\n",
    "Load the engineered features from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e1bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to features file\n",
    "PROCESSED = os.path.abspath(os.path.join('..', 'data', 'processed'))\n",
    "features_path = os.path.join(PROCESSED, 'features.csv')\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(features_path):\n",
    "    print(f'ERROR: features.csv not found at {features_path}')\n",
    "    print('Please run notebook 03_feature_engineering.ipynb first to generate the features file.')\n",
    "else:\n",
    "    df = pd.read_csv(features_path)\n",
    "    print(f'Features loaded successfully from {features_path}')\n",
    "    print(f'Dataset shape: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48db83cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aae807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check column names and data types\n",
    "print('Columns in dataset:')\n",
    "print(df.columns.tolist())\n",
    "print(f'\\nData types:')\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856af26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print('Missing values per column:')\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd456fd1",
   "metadata": {},
   "source": [
    "## 2. Define Features and Target\n",
    "\n",
    "Separate the features (X) from the target label (y). The target is typically named 'is_laundering', 'is_fraud', or similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434e30b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify target column (common names for AML datasets)\n",
    "possible_target_names = ['is_laundering', 'is_fraud', 'label', 'target', 'fraud', 'laundering']\n",
    "target_col = None\n",
    "\n",
    "for col in possible_target_names:\n",
    "    if col in df.columns:\n",
    "        target_col = col\n",
    "        break\n",
    "\n",
    "if target_col is None:\n",
    "    print('ERROR: Could not find target column. Please specify manually.')\n",
    "    print(f'Available columns: {df.columns.tolist()}')\n",
    "else:\n",
    "    print(f'Target column identified: {target_col}')\n",
    "    \n",
    "    # Check class distribution\n",
    "    print(f'\\nClass distribution:')\n",
    "    print(df[target_col].value_counts())\n",
    "    print(f'\\nClass proportions:')\n",
    "    print(df[target_col].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f599b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "# Exclude target column and any ID columns\n",
    "exclude_cols = [target_col, 'id', 'transaction_id', 'account_id', 'customer_id']\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[target_col]\n",
    "\n",
    "print(f'Features (X) shape: {X.shape}')\n",
    "print(f'Target (y) shape: {y.shape}')\n",
    "print(f'\\nFeature columns ({len(feature_cols)}):')\n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5583e6",
   "metadata": {},
   "source": [
    "## 3. Stratified Train/Test Split\n",
    "\n",
    "Split the data into training and testing sets using stratified sampling to maintain class proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd73c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stratified train/test split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f'Training set size: {X_train.shape[0]} samples')\n",
    "print(f'Test set size: {X_test.shape[0]} samples')\n",
    "print(f'\\nTraining set class distribution:')\n",
    "print(y_train.value_counts())\n",
    "print(f'\\nTest set class distribution:')\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80668edf",
   "metadata": {},
   "source": [
    "## 4. Baseline Model 1: Logistic Regression\n",
    "\n",
    "Train a simple logistic regression model as our first baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd239308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "print('Training Logistic Regression...')\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "print('Training complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c94a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "lr_precision = precision_score(y_test, y_pred_lr)\n",
    "lr_recall = recall_score(y_test, y_pred_lr)\n",
    "lr_f1 = f1_score(y_test, y_pred_lr)\n",
    "\n",
    "print('=' * 60)\n",
    "print('LOGISTIC REGRESSION - EVALUATION METRICS')\n",
    "print('=' * 60)\n",
    "print(f'Accuracy:  {lr_accuracy:.4f}')\n",
    "print(f'Precision: {lr_precision:.4f}')\n",
    "print(f'Recall:    {lr_recall:.4f}')\n",
    "print(f'F1 Score:  {lr_f1:.4f}')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceb99da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed classification report\n",
    "print('\\nDetailed Classification Report:')\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec94022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "lr_cm = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix - Logistic Regression')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nConfusion Matrix:')\n",
    "print(f'True Negatives:  {lr_cm[0, 0]}')\n",
    "print(f'False Positives: {lr_cm[0, 1]}')\n",
    "print(f'False Negatives: {lr_cm[1, 0]}')\n",
    "print(f'True Positives:  {lr_cm[1, 1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276eea07",
   "metadata": {},
   "source": [
    "## 5. Baseline Model 2: Decision Tree\n",
    "\n",
    "Train a decision tree classifier as our second baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d9ff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Decision Tree\n",
    "print('Training Decision Tree...')\n",
    "dt_model = DecisionTreeClassifier(random_state=42, max_depth=10)\n",
    "dt_model.fit(X_train, y_train)\n",
    "print('Training complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a63e844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
    "dt_precision = precision_score(y_test, y_pred_dt)\n",
    "dt_recall = recall_score(y_test, y_pred_dt)\n",
    "dt_f1 = f1_score(y_test, y_pred_dt)\n",
    "\n",
    "print('=' * 60)\n",
    "print('DECISION TREE - EVALUATION METRICS')\n",
    "print('=' * 60)\n",
    "print(f'Accuracy:  {dt_accuracy:.4f}')\n",
    "print(f'Precision: {dt_precision:.4f}')\n",
    "print(f'Recall:    {dt_recall:.4f}')\n",
    "print(f'F1 Score:  {dt_f1:.4f}')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3a422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed classification report\n",
    "print('\\nDetailed Classification Report:')\n",
    "print(classification_report(y_test, y_pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b913da9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "dt_cm = confusion_matrix(y_test, y_pred_dt)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(dt_cm, annot=True, fmt='d', cmap='Greens', cbar=False)\n",
    "plt.title('Confusion Matrix - Decision Tree')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nConfusion Matrix:')\n",
    "print(f'True Negatives:  {dt_cm[0, 0]}')\n",
    "print(f'False Positives: {dt_cm[0, 1]}')\n",
    "print(f'False Negatives: {dt_cm[1, 0]}')\n",
    "print(f'True Positives:  {dt_cm[1, 1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3ca324",
   "metadata": {},
   "source": [
    "## 6. Compare Baseline Models\n",
    "\n",
    "Quick comparison of both baseline models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a077aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Decision Tree'],\n",
    "    'Accuracy': [lr_accuracy, dt_accuracy],\n",
    "    'Precision': [lr_precision, dt_precision],\n",
    "    'Recall': [lr_recall, dt_recall],\n",
    "    'F1 Score': [lr_f1, dt_f1]\n",
    "})\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('BASELINE MODELS COMPARISON')\n",
    "print('=' * 80)\n",
    "print(comparison.to_string(index=False))\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5497e7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(x - width/2, [lr_accuracy, lr_precision, lr_recall, lr_f1], width, label='Logistic Regression', alpha=0.8)\n",
    "ax.bar(x + width/2, [dt_accuracy, dt_precision, dt_recall, dt_f1], width, label='Decision Tree', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Baseline Models Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96bdbef",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have successfully built and evaluated two baseline models:\n",
    "\n",
    "1. **Logistic Regression**: Simple linear model that provides a good starting point\n",
    "2. **Decision Tree**: Non-linear model that can capture more complex patterns\n",
    "\n",
    "**Key Observations:**\n",
    "- These baseline models give us a performance benchmark to improve upon\n",
    "- In AML detection, **recall** is often more important than precision (we want to catch as many fraudulent transactions as possible)\n",
    "- The class imbalance may be affecting model performance\n",
    "\n",
    "**Next Steps:**\n",
    "- Handle class imbalance using techniques like class weighting or resampling\n",
    "- Try more advanced models (Random Forest, XGBoost)\n",
    "- Tune decision thresholds to optimize for recall"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
