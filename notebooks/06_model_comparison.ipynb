{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b7548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# Try to import XGBoost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print('XGBoost not available. Will compare available models only.')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print('Libraries imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4913e5",
   "metadata": {},
   "source": [
    "## 1. Load Data and Prepare Train/Test Split\n",
    "\n",
    "Use the same train/test split as all previous notebooks for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75991663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features\n",
    "PROCESSED = os.path.abspath(os.path.join('..', 'data', 'processed'))\n",
    "features_path = os.path.join(PROCESSED, 'features.csv')\n",
    "\n",
    "if not os.path.exists(features_path):\n",
    "    print(f'ERROR: features.csv not found at {features_path}')\n",
    "else:\n",
    "    df = pd.read_csv(features_path)\n",
    "    print(f'Features loaded successfully')\n",
    "    print(f'Dataset shape: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ef76c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify target column\n",
    "possible_target_names = ['is_laundering', 'is_fraud', 'label', 'target', 'fraud', 'laundering']\n",
    "target_col = None\n",
    "\n",
    "for col in possible_target_names:\n",
    "    if col in df.columns:\n",
    "        target_col = col\n",
    "        break\n",
    "\n",
    "print(f'Target column: {target_col}')\n",
    "print(f'\\nClass distribution:')\n",
    "print(df[target_col].value_counts())\n",
    "print(f'\\nClass proportions:')\n",
    "print(df[target_col].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c22b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "exclude_cols = [target_col, 'id', 'transaction_id', 'account_id', 'customer_id']\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[target_col]\n",
    "\n",
    "print(f'Features shape: {X.shape}')\n",
    "print(f'Target shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70037a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified train/test split (same as all previous notebooks)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f'Training set: {X_train.shape[0]} samples')\n",
    "print(f'Test set: {X_test.shape[0]} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b84d9d4",
   "metadata": {},
   "source": [
    "## 2. Train All Models\n",
    "\n",
    "Train each model with their best configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d0c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store models and results\n",
    "models = {}\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a8995f",
   "metadata": {},
   "source": [
    "### Model 1: Logistic Regression (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd20e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Logistic Regression...')\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "y_pred_proba_lr = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Store model\n",
    "models['Logistic Regression'] = lr_model\n",
    "\n",
    "# Calculate metrics\n",
    "results.append({\n",
    "    'Model': 'Logistic Regression',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_lr),\n",
    "    'Precision': precision_score(y_test, y_pred_lr),\n",
    "    'Recall': recall_score(y_test, y_pred_lr),\n",
    "    'F1 Score': f1_score(y_test, y_pred_lr),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_pred_proba_lr)\n",
    "})\n",
    "\n",
    "print('Logistic Regression complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f032a2",
   "metadata": {},
   "source": [
    "### Model 2: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4393f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Decision Tree...')\n",
    "dt_model = DecisionTreeClassifier(random_state=42, max_depth=10, class_weight='balanced')\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "y_pred_proba_dt = dt_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Store model\n",
    "models['Decision Tree'] = dt_model\n",
    "\n",
    "# Calculate metrics\n",
    "results.append({\n",
    "    'Model': 'Decision Tree',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_dt),\n",
    "    'Precision': precision_score(y_test, y_pred_dt),\n",
    "    'Recall': recall_score(y_test, y_pred_dt),\n",
    "    'F1 Score': f1_score(y_test, y_pred_dt),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_pred_proba_dt)\n",
    "})\n",
    "\n",
    "print('Decision Tree complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32e159f",
   "metadata": {},
   "source": [
    "### Model 3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3ad6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Random Forest...')\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Store model\n",
    "models['Random Forest'] = rf_model\n",
    "\n",
    "# Calculate metrics\n",
    "results.append({\n",
    "    'Model': 'Random Forest',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_rf),\n",
    "    'Precision': precision_score(y_test, y_pred_rf),\n",
    "    'Recall': recall_score(y_test, y_pred_rf),\n",
    "    'F1 Score': f1_score(y_test, y_pred_rf),\n",
    "    'ROC-AUC': roc_auc_score(y_test, y_pred_proba_rf)\n",
    "})\n",
    "\n",
    "print('Random Forest complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265c4231",
   "metadata": {},
   "source": [
    "### Model 4: XGBoost (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49304a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "if XGBOOST_AVAILABLE:\n",
    "    print('Training XGBoost...')\n",
    "    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_xgb = xgb_model.predict(X_test)\n",
    "    y_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Store model\n",
    "    models['XGBoost'] = xgb_model\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results.append({\n",
    "        'Model': 'XGBoost',\n",
    "        'Accuracy': accuracy_score(y_test, y_pred_xgb),\n",
    "        'Precision': precision_score(y_test, y_pred_xgb),\n",
    "        'Recall': recall_score(y_test, y_pred_xgb),\n",
    "        'F1 Score': f1_score(y_test, y_pred_xgb),\n",
    "        'ROC-AUC': roc_auc_score(y_test, y_pred_proba_xgb)\n",
    "    })\n",
    "    \n",
    "    print('XGBoost complete.')\n",
    "else:\n",
    "    print('XGBoost not available. Skipping.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07287d99",
   "metadata": {},
   "source": [
    "## 3. Complete Model Comparison Table\n",
    "\n",
    "Display all metrics for all models in one comprehensive table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4458009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame(results)\n",
    "\n",
    "print('\\n' + '=' * 95)\n",
    "print('COMPLETE MODEL COMPARISON')\n",
    "print('=' * 95)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print('=' * 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe571b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best models for each metric\n",
    "print('\\n' + '=' * 60)\n",
    "print('BEST MODEL FOR EACH METRIC')\n",
    "print('=' * 60)\n",
    "print(f\"Best Accuracy:  {comparison_df.loc[comparison_df['Accuracy'].idxmax(), 'Model']} ({comparison_df['Accuracy'].max():.4f})\")\n",
    "print(f\"Best Precision: {comparison_df.loc[comparison_df['Precision'].idxmax(), 'Model']} ({comparison_df['Precision'].max():.4f})\")\n",
    "print(f\"Best Recall:    {comparison_df.loc[comparison_df['Recall'].idxmax(), 'Model']} ({comparison_df['Recall'].max():.4f})\")\n",
    "print(f\"Best F1 Score:  {comparison_df.loc[comparison_df['F1 Score'].idxmax(), 'Model']} ({comparison_df['F1 Score'].max():.4f})\")\n",
    "print(f\"Best ROC-AUC:   {comparison_df.loc[comparison_df['ROC-AUC'].idxmax(), 'Model']} ({comparison_df['ROC-AUC'].max():.4f})\")\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08569aba",
   "metadata": {},
   "source": [
    "## 4. Visual Comparison - All Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a57b53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparing all metrics\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.18  # Width of bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Plot bars for each model\n",
    "for i, model_name in enumerate(comparison_df['Model']):\n",
    "    model_scores = comparison_df.iloc[i][metrics].values\n",
    "    offset = (i - len(comparison_df) / 2 + 0.5) * width\n",
    "    ax.bar(x + offset, model_scores, width, label=model_name, alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance Comparison - All Metrics')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1.05])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3dd953",
   "metadata": {},
   "source": [
    "## 5. Focus on Key Metrics for AML\n",
    "\n",
    "For fraud detection, **Recall** and **F1 Score** are most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cb0ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on Recall and F1 Score\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Recall\n",
    "axes[0].bar(comparison_df['Model'], comparison_df['Recall'], alpha=0.8, color='steelblue')\n",
    "axes[0].set_ylabel('Recall')\n",
    "axes[0].set_title('Recall Comparison (Most Important for AML)')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# F1 Score\n",
    "axes[1].bar(comparison_df['Model'], comparison_df['F1 Score'], alpha=0.8, color='coral')\n",
    "axes[1].set_ylabel('F1 Score')\n",
    "axes[1].set_title('F1 Score Comparison (Balance of Precision & Recall)')\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].tick_params(axis='x', rotation=15)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27530828",
   "metadata": {},
   "source": [
    "## 6. Radar Chart for Holistic View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0269113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar chart to visualize all metrics together\n",
    "from math import pi\n",
    "\n",
    "# Number of metrics\n",
    "categories = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
    "N = len(categories)\n",
    "\n",
    "# Create angles for each metric\n",
    "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Initialize plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "# Plot each model\n",
    "colors = ['blue', 'green', 'red', 'orange', 'purple']\n",
    "for i, model_name in enumerate(comparison_df['Model']):\n",
    "    values = comparison_df.iloc[i][categories].values.tolist()\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors[i % len(colors)])\n",
    "    ax.fill(angles, values, alpha=0.15, color=colors[i % len(colors)])\n",
    "\n",
    "# Fix axis\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'])\n",
    "ax.grid(True)\n",
    "\n",
    "plt.title('Model Performance Radar Chart', size=16, y=1.08)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fea3b9",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrices Side-by-Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016adb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for all models\n",
    "predictions = {\n",
    "    'Logistic Regression': y_pred_lr,\n",
    "    'Decision Tree': y_pred_dt,\n",
    "    'Random Forest': y_pred_rf\n",
    "}\n",
    "\n",
    "if XGBOOST_AVAILABLE:\n",
    "    predictions['XGBoost'] = y_pred_xgb\n",
    "\n",
    "# Create confusion matrices\n",
    "n_models = len(predictions)\n",
    "fig, axes = plt.subplots(1, n_models, figsize=(5 * n_models, 4))\n",
    "\n",
    "if n_models == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, (model_name, y_pred) in enumerate(predictions.items()):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[i])\n",
    "    axes[i].set_title(f'{model_name}')\n",
    "    axes[i].set_ylabel('Actual')\n",
    "    axes[i].set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a18e87",
   "metadata": {},
   "source": [
    "## 8. Detailed Analysis and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357eb35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking models by different criteria\n",
    "print('\\n' + '=' * 60)\n",
    "print('MODEL RANKINGS')\n",
    "print('=' * 60)\n",
    "\n",
    "print('\\n1. By Recall (Best for catching fraud):')\n",
    "recall_ranking = comparison_df.sort_values('Recall', ascending=False)[['Model', 'Recall']]\n",
    "for idx, row in recall_ranking.iterrows():\n",
    "    print(f\"   {idx + 1}. {row['Model']}: {row['Recall']:.4f}\")\n",
    "\n",
    "print('\\n2. By F1 Score (Best overall balance):')\n",
    "f1_ranking = comparison_df.sort_values('F1 Score', ascending=False)[['Model', 'F1 Score']]\n",
    "for idx, row in f1_ranking.iterrows():\n",
    "    print(f\"   {idx + 1}. {row['Model']}: {row['F1 Score']:.4f}\")\n",
    "\n",
    "print('\\n3. By ROC-AUC (Best discrimination ability):')\n",
    "auc_ranking = comparison_df.sort_values('ROC-AUC', ascending=False)[['Model', 'ROC-AUC']]\n",
    "for idx, row in auc_ranking.iterrows():\n",
    "    print(f\"   {idx + 1}. {row['Model']}: {row['ROC-AUC']:.4f}\")\n",
    "\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310b80b6",
   "metadata": {},
   "source": [
    "## Summary and Recommendations\n",
    "\n",
    "### Model Comparison Results:\n",
    "\n",
    "**1. Logistic Regression (Baseline)**\n",
    "- Simple, interpretable model\n",
    "- Fast training and prediction\n",
    "- Good starting point but may underperform on complex patterns\n",
    "- Best when: You need model interpretability and simplicity\n",
    "\n",
    "**2. Decision Tree**\n",
    "- Non-linear model that captures interactions\n",
    "- Easy to interpret with feature importance\n",
    "- May overfit without proper pruning\n",
    "- Best when: You need interpretable non-linear patterns\n",
    "\n",
    "**3. Random Forest**\n",
    "- Ensemble of trees - more robust than single tree\n",
    "- Reduces overfitting while maintaining performance\n",
    "- Provides feature importance\n",
    "- Best when: You want robust performance with some interpretability\n",
    "\n",
    "**4. XGBoost** (if available)\n",
    "- State-of-the-art gradient boosting\n",
    "- Often achieves best performance\n",
    "- Handles complex patterns well\n",
    "- Best when: Maximum performance is priority\n",
    "\n",
    "### For AML Detection, Key Priorities:\n",
    "\n",
    "1. **Recall**: Most important - we must catch fraud cases\n",
    "2. **F1 Score**: Good balance between precision and recall\n",
    "3. **ROC-AUC**: Overall discriminative ability\n",
    "\n",
    "### Best Model Overall:\n",
    "\n",
    "Based on the comparison:\n",
    "- **Best for detecting fraud**: Model with highest recall\n",
    "- **Best overall performance**: Model with highest F1 score\n",
    "- **Best for production**: Balance of performance, speed, and interpretability\n",
    "\n",
    "### Trade-offs to Consider:\n",
    "\n",
    "- **High Recall** = More fraud detected, but more false alarms (legitimate transactions flagged)\n",
    "- **High Precision** = Fewer false alarms, but risk missing some fraud cases\n",
    "- **F1 Score** = Best balance between the two\n",
    "\n",
    "### Recommendation:\n",
    "\n",
    "Select the model with the **highest recall** if catching all fraud is critical, or the **highest F1 score** if you need a balanced approach. The selected model will be saved and used for final deployment in the next notebook."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
